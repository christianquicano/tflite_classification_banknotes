{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom TransferLearning using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "#!pip install imutils\n",
    "#!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyimagesearch.io import HDF5DatasetWriter\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import argparse\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parámetros de la extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "batch_size = 32\n",
    "buffer_size = 1000 #control the number of extracted feature store in memory\n",
    "path_train = 'data_' + str(img_size) + '/train'\n",
    "path_validation = 'data_' + str(img_size) + '/validation'\n",
    "path_output_train_hdf5 = 'HDF5/VGG16_train.hdf5'\n",
    "path_output_validation_hdf5 = 'HDF5/VGG16_validation.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de la data para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargando las imágenes\n",
    "imagePaths_train = list(paths.list_images(path_train))\n",
    "imagePaths_validation = list(paths.list_images(path_validation))\n",
    "#moviendo de forma aleatoria el orden de las imágenes\n",
    "random.shuffle(imagePaths_train)\n",
    "random.shuffle(imagePaths_validation)\n",
    "\n",
    "#Obteniendo la etiqueta de la imagen\n",
    "labels_train = [p.split(os.path.sep)[-2] for p in imagePaths_train]\n",
    "#Códificando las etiquetas en númerops\n",
    "le = LabelEncoder()\n",
    "labels_train = le.fit_transform(labels_train)\n",
    "\n",
    "#Obteniendo la etiqueta de la imagen\n",
    "labels_validation = [p.split(os.path.sep)[-2] for p in imagePaths_validation]\n",
    "#Códificando las etiquetas en númerops\n",
    "le = LabelEncoder()\n",
    "labels_validation = le.fit_transform(labels_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 7s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(img_size,img_size,3)\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creando el archivo hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = HDF5DatasetWriter((len(imagePaths_train), 512 * 7 * 7), #es igual al tamaño de la última capa\n",
    "                            path_output_train_hdf5,\n",
    "                            dataKey = \"features\",\n",
    "                            bufSize = buffer_size)\n",
    "dataset_train.storeClassLabels(le.classes_)\n",
    "\n",
    "dataset_validation = HDF5DatasetWriter((len(imagePaths_validation), 512 * 7 * 7), #es igual al tamaño de la última capa\n",
    "                            path_output_validation_hdf5,\n",
    "                            dataKey = \"features\",\n",
    "                            bufSize = buffer_size)\n",
    "dataset_validation.storeClassLabels(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creando la extracción de la data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracción de características: 100% |###########################| Time: 0:01:47\n"
     ]
    }
   ],
   "source": [
    "widgets = [\"Extracción de características: \",\n",
    "           progressbar.Percentage(),\n",
    "           \" \",\n",
    "           progressbar.Bar(),\n",
    "           \" \",\n",
    "           progressbar.ETA()]\n",
    "pbar = progressbar.ProgressBar(maxval=len(imagePaths_train),\n",
    "                               widgets=widgets).start()\n",
    "\n",
    "# recorriendo el array con una distancia del batch_size\n",
    "for i in np.arange(0, len(imagePaths_train), batch_size):\n",
    "    # obteniendo los paths y labels de las imágenes\n",
    "    batchPaths = imagePaths_train[i:i + batch_size]\n",
    "    batchLabels = labels_train[i:i + batch_size]\n",
    "    batchImages = []\n",
    "    # recorremos el bath obtenido\n",
    "    for (j, imagePath) in enumerate(batchPaths):\n",
    "        #cargamos las imagenes con keras\n",
    "        image = load_img(imagePath, target_size=(img_size, img_size))\n",
    "        image = img_to_array(image)\n",
    "        \n",
    "        #hacemos un pre-procesamiento de la imagen\n",
    "        #le agregamos una dimensión\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        #obteniendo la media de la intesidad RGB del pixel\n",
    "        image = imagenet_utils.preprocess_input(image)\n",
    "        \n",
    "        #agregando la imagen al batch\n",
    "        batchImages.append(image)\n",
    "        \n",
    "    #creamos un stack con el array\n",
    "    batchImages = np.vstack(batchImages)\n",
    "    features = model.predict(batchImages, batch_size=batch_size)\n",
    "    \n",
    "    #hacemos un reshape de la predicción, convirtiendolo en 1D\n",
    "    features = features.reshape((features.shape[0], 512 * 7 * 7))\n",
    "    \n",
    "    #agregamps los features en  el HDF5\n",
    "    dataset_train.add(features, batchLabels)\n",
    "    pbar.update(i)\n",
    "        \n",
    "#cerramos el  HDF5\n",
    "dataset_train.close()\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracción de características: 100% |###########################| Time: 0:00:06\n"
     ]
    }
   ],
   "source": [
    "widgets = [\"Extracción de características: \",\n",
    "           progressbar.Percentage(),\n",
    "           \" \",\n",
    "           progressbar.Bar(),\n",
    "           \" \",\n",
    "           progressbar.ETA()]\n",
    "pbar = progressbar.ProgressBar(maxval=len(imagePaths_validation),\n",
    "                               widgets=widgets).start()\n",
    "\n",
    "# recorriendo el array con una distancia del batch_size\n",
    "for i in np.arange(0, len(imagePaths_validation), batch_size):\n",
    "    # obteniendo los paths y labels de las imágenes\n",
    "    batchPaths = imagePaths_validation[i:i + batch_size]\n",
    "    batchLabels = labels_validation[i:i + batch_size]\n",
    "    batchImages = []\n",
    "    # recorremos el bath obtenido\n",
    "    for (j, imagePath) in enumerate(batchPaths):\n",
    "        #cargamos las imagenes con keras\n",
    "        image = load_img(imagePath, target_size=(img_size, img_size))\n",
    "        image = img_to_array(image)\n",
    "        \n",
    "        #hacemos un pre-procesamiento de la imagen\n",
    "        #le agregamos una dimensión\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        #obteniendo la media de la intesidad RGB del pixel\n",
    "        image = imagenet_utils.preprocess_input(image)\n",
    "        \n",
    "        #agregando la imagen al batch\n",
    "        batchImages.append(image)\n",
    "        \n",
    "    #creamos un stack con el array\n",
    "    batchImages = np.vstack(batchImages)\n",
    "    features = model.predict(batchImages, batch_size=batch_size)\n",
    "    \n",
    "    #hacemos un reshape de la predicción, convirtiendolo en 1D\n",
    "    features = features.reshape((features.shape[0], 512 * 7 * 7))\n",
    "    \n",
    "    #agregamps los features en  el HDF5\n",
    "    dataset_validation.add(features, batchLabels)\n",
    "    pbar.update(i)\n",
    "        \n",
    "#cerramos el  HDF5\n",
    "dataset_validation.close()\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando un clasificador con las características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import argparse\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leemos el HDF5\n",
    "db_train = h5py.File(path_output_train_hdf5, 'r')\n",
    "db_validation = h5py.File(path_output_validation_hdf5, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23975, 25088)\n",
      "(23975,)\n"
     ]
    }
   ],
   "source": [
    "print(db_train[\"features\"].shape)\n",
    "print(db_train[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/backend/queues.py\", line 150, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/backend/reduction.py\", line 247, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/backend/reduction.py\", line 240, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"/usr/local/lib/python3.6/dist-packages/joblib/externals/cloudpickle/cloudpickle.py\", line 477, in dump\n    return Pickler.dump(self, obj)\n  File \"/usr/lib/python3.6/pickle.py\", line 409, in dump\n    self.save(obj)\n  File \"/usr/lib/python3.6/pickle.py\", line 521, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/lib/python3.6/pickle.py\", line 634, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.6/pickle.py\", line 521, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/lib/python3.6/pickle.py\", line 634, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.6/pickle.py\", line 852, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.6/pickle.py\", line 521, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/lib/python3.6/pickle.py\", line 610, in save_reduce\n    save(args)\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.6/pickle.py\", line 751, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.6/pickle.py\", line 808, in _batch_appends\n    save(tmp[0])\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.6/pickle.py\", line 736, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.6/pickle.py\", line 736, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.6/pickle.py\", line 496, in save\n    rv = reduce(self.proto)\n  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/base.py\", line 308, in __getnewargs__\n    raise TypeError(\"h5py objects cannot be pickled\")\nTypeError: h5py objects cannot be pickled\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b9e26ae9751c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                         \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                         n_jobs=-1)\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] mejores hipeparametros: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    713\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 715\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    538\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "#definiendo los hiperparametros\n",
    "params = {\"C\": [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]}\n",
    "model = GridSearchCV(LogisticRegression(solver=\"lbfgs\",\n",
    "                                        multi_class=\"auto\"),\n",
    "                                        params,\n",
    "                                        cv=3,\n",
    "                                        n_jobs=-1)\n",
    "model.fit(db_train[\"features\"], db_train[\"labels\"])\n",
    "print(\"[INFO] mejores hipeparametros: {}\".format(model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando un gráfico para la revisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "plt.ylim([0,1.1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "#plt.ylim([0,1.0])\n",
    "plt.ylim([0,20])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = 'model_MobileNetV2'\n",
    "tf.saved_model.save(model, saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convirtiendo el modelo en tensorflow lite para el uso en mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "path_tensorflow_lite = 'model_tflite/model_MobileNetV2.tflite'\n",
    "with open(path_tensorflow_lite, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomando 25 imágenes de validación y observando el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ImageClassifierDataLoader.from_folder(path_test_validation)\n",
    "\n",
    "def get_label_color(val1, val2):\n",
    "  if val1 == val2:\n",
    "    return 'black'\n",
    "  else:\n",
    "    return 'red'\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i, (image, label) in enumerate(test_data.dataset.take(100)):\n",
    "    ax = plt.subplot(10, 10, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(image.numpy(), cmap=plt.cm.gray)\n",
    "\n",
    "    swapped = np.moveaxis(image, 0, 1)\n",
    "    arr4d = np.expand_dims(swapped, 0)\n",
    "    id_predict = np.argmax(model.predict(arr4d))\n",
    "    \n",
    "    predict_label = \"100\"\n",
    "    if id_predict == 1:\n",
    "        predict_label = \"20\"\n",
    "    color = get_label_color(predict_label, test_data.index_to_label[label.numpy()])\n",
    "    #color = 'black'\n",
    "    ax.xaxis.label.set_color(color)\n",
    "    plt.xlabel('Predicted: %s' % predict_label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
